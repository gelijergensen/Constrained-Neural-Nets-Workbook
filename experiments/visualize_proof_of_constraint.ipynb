{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to the root so that relative path loads work correctly\n",
    "import os\n",
    "\n",
    "try:\n",
    "    os.chdir(os.path.join(os.getcwd(), \"..\"))\n",
    "    print(os.getcwd())\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from experiments.A_proof_of_constraint.main import build_model_and_optimizer, get_data\n",
    "from experiments.A_proof_of_constraint.visualize import (\n",
    "    plot_constraints,\n",
    "    plot_loss,\n",
    "    plot_model_predictions,\n",
    "    plot_time,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predictions(checkpoints):\n",
    "    # Retrieve the data and equation of the first checkpoint\n",
    "    train_dl, test_dl, parameterization, equation = get_data(\n",
    "        checkpoints[0][\"configuration\"], return_equation=True\n",
    "    )\n",
    "    # Get the models\n",
    "    models = list()\n",
    "    for checkpoint in checkpoints:\n",
    "        model, opt = build_model_and_optimizer(checkpoint[\"configuration\"])\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        models.append(model)\n",
    "    # Get the predictions\n",
    "    inputs = list()\n",
    "    outputs = list()\n",
    "    is_training = list()\n",
    "    predictions = [list() for _ in models]\n",
    "    for xb, yb in train_dl:\n",
    "        inputs.extend(xb.squeeze().detach().numpy())\n",
    "        outputs.extend(yb.squeeze().detach().numpy())\n",
    "        is_training.extend([True for _ in range(len(xb.squeeze()))])\n",
    "        for i, model in enumerate(models):\n",
    "            model.eval()\n",
    "            predictions[i].extend(model(xb).squeeze().detach().numpy())\n",
    "    for xb, yb in test_dl:\n",
    "        inputs.extend(xb.squeeze().detach().numpy())\n",
    "        outputs.extend(yb.squeeze().detach().numpy())\n",
    "        is_training.extend([False for _ in range(len(xb.squeeze()))])\n",
    "        for i, model in enumerate(models):\n",
    "            model.eval()\n",
    "            predictions[i].extend(model(xb).squeeze().detach().numpy())\n",
    "    # sort\n",
    "    idxs = np.argsort(inputs)\n",
    "    inputs = np.array(inputs)[idxs]\n",
    "    outputs = np.array(outputs)[idxs]\n",
    "    is_training = np.array(is_training)[idxs]\n",
    "    predictions = np.array(predictions)[:, idxs]\n",
    "    return (inputs, outputs, is_training), predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_name_to_filename(model_name):\n",
    "    filename = model_name.replace(\" \", \"-\").lower()\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(checkpoint):\n",
    "    config = checkpoint[\"configuration\"]\n",
    "    sampling = config[\"training_sampling\"]\n",
    "    method = config[\"method\"]\n",
    "    model_act = config[\"model_act\"]\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    task = \"Ext\" if sampling == \"start\" else \"Int\"\n",
    "    return f\"{method} {str(model_act)[:-2]} ({task} {epoch})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_special_model_name(checkpoint, filename):\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    suffix = \"Before\" if epoch <= 200 else \"After\"\n",
    "    if \"2019-08-05-15-07\" in filename:\n",
    "        return f\"Unconstrained --> Constrained ({suffix})\"\n",
    "    elif \"2019-08-05-15-05\" in filename:\n",
    "        return f\"Unconstrained --> Reduction ({suffix})\"\n",
    "    elif \"2019-08-05-14-38\" in filename:\n",
    "        return f\"Unconstrained --> No-loss ({suffix})\"\n",
    "    else:\n",
    "        return get_model_name(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files to load\n",
    "experiment_name = \"A_proof_of_concept\"\n",
    "save_directory = f\"/global/u1/g/gelijerg/Projects/pyinsulate/results/{experiment_name}/\"\n",
    "# load_directory = os.path.expandvars(\n",
    "#     \"$SCRATCH/clones/20190731-160459/pyinsulate/results/checkpoints/\"\n",
    "# )\n",
    "# checkpoint_pattern = \"*.pth\"\n",
    "load_directory = os.path.expandvars(\"results/checkpoints/\")\n",
    "# checkpoint_patterns = [\"proof-of-constraint_2019-08-02-15-34-*_0???0.pth\"] # 50 epoch test of frequency = 1\n",
    "# checkpoint_patterns = [\"proof-of-constraint_2019-08-05-08-5[3|4]-*_0???0.pth\"] # 100 epoch test of frequency = 5, amplitude = 0.04\n",
    "# checkpoint_patterns = [ \"proof-of-constraint_2019-08-05-08-59-*_0???0.pth\", \"proof-of-constraint_2019-08-05-09-00-*_0???0.pth\"] # 100 epoch test of frequency = 0.2, amplitude = 0.04\n",
    "# checkpoint_patterns = [ \"proof-of-constraint_2019-08-05-09-0[4|5|7]-*_0???0.pth\"] # 300 epoch test of frequency = 5\n",
    "# checkpoint_patterns = [ \"proof-of-constraint_2019-08-05-09-1[4|5]-*_0???0.pth\"] # 100 epoch test of frequency = 5 after PDE modification\n",
    "# checkpoint_patterns = [\"proof-of-constraint_2019-08-05-09-59-*_0??[0|5]0.pth\", \"proof-of-constraint_2019-08-05-10-0?-*_0??[0|5]0.pth\"]\n",
    "# checkpoint_patterns = [\"proof-of-constraint_2019-08-05-11-29-*_0???0.pth\"] # clamp = 5\n",
    "# checkpoint_patterns = [\"proof-of-constraint_2019-08-05-11-32-*_0???0.pth\"]  # clamp = 1\n",
    "# checkpoint_patterns = [\"proof-of-constraint_2019-08-05-11-3[8|9]-*_0???0.pth\"]  # revised data, clamp = 1\n",
    "# checkpoint_patterns = [\"proof-of-constraint_2019-08-05-11-42-*_0???0.pth\"]  # revised data, clamp = None\n",
    "# checkpoint_patterns = [\"proof-of-constraint_2019-08-05-11-4[8|9]-*_0???0.pth\"]  # revised data, clamp = \"log\"\n",
    "# checkpoint_patterns = [\"proof-of-constraint_2019-08-05-11-53-*_0???0.pth\"]  # revised data, clamp = x : 1+x\n",
    "# checkpoint_patterns = [\"proof-of-constraint_2019-08-05-11-5[8|9]-*_0???0.pth\"]  # revised data, Huber(6)\n",
    "# checkpoint_patterns = [\"proof-of-constraint_2019-08-05-12-0[1|2|6]-*_0???0.pth\"]  # revised data, Huber(6), 500 epochs\n",
    "# checkpoint_patterns = [\"proof-of-constraint_2019-08-05-12-1[5|6]-*_0???0.pth\", \"proof-of-constraint_2019-08-05-12-20-*_0???0.pth\"]  # revised data, Huber(6), tanh\n",
    "# checkpoint_patterns = [\"proof-of-constraint_2019-08-05-12-3[1|2]-*_0???0.pth\"]  # revised data, Huber(6), 100 epochs\n",
    "checkpoint_patterns = [\"proof-of-constraint_2019-08-05-12-4[1|2]-*_0???0.pth\"]\n",
    "checkpoint_patterns = [\"proof-of-constraint_2019-08-05-12-5[3|4]-*_0???0.pth\"]\n",
    "checkpoint_patterns = [\"proof-of-constraint_2019-08-05-14-23-28_0???0.pth\"]\n",
    "checkpoint_patterns = [\n",
    "    #                         'proof-of-constraint_2019-08-05-14-38-*_0???0.pth', # 200 epochs unconstrained followed by 200 epochs no-loss\n",
    "    #                       'proof-of-constraint_2019-08-05-15-05-*_0???0.pth', # 200 epochs unconstrained followed by 200 epochs reduction\n",
    "    #                       'proof-of-constraint_2019-08-05-15-07-*_0???0.pth', # 200 epochs unconstrained followed by 200 epochs constrained\n",
    "    \"proof-of-constraint_2019-08-05-15-38-*_0???0.pth\",  # 400 epochs unconstrained\n",
    "    \"proof-of-constraint_2019-08-05-15-39-*_0???0.pth\",  # 400 epochs reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "files = list()\n",
    "for pattern in checkpoint_patterns:\n",
    "    files.extend(glob.glob(f\"{load_directory}/{pattern}\"))\n",
    "files.sort()\n",
    "checkpoints = [torch.load(f) for f in files]\n",
    "# model_names = [get_model_name(checkpoint) for checkpoint in checkpoints]\n",
    "model_names = [\n",
    "    get_special_model_name(checkpoint, filename)\n",
    "    for checkpoint, filename in zip(checkpoints, files)\n",
    "]\n",
    "# Make sure directory to save exists\n",
    "os.makedirs(save_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some plotting\n",
    "max_epoch = max([checkpoint[\"epoch\"] for checkpoint in checkpoints])\n",
    "final_checkpoints = [\n",
    "    (checkpoint, model_name)\n",
    "    for (checkpoint, model_name) in zip(checkpoints, model_names)\n",
    "    if checkpoint[\"epoch\"] == max_epoch or checkpoint[\"epoch\"] == 190\n",
    "]\n",
    "approximation_checkpoints = [\n",
    "    (checkpoint, model_name)\n",
    "    for (checkpoint, model_name) in zip(checkpoints, model_names)\n",
    "    if checkpoint[\"configuration\"][\"method\"] == \"approximate\"\n",
    "]\n",
    "reduction_checkpoints = [\n",
    "    (checkpoint, model_name)\n",
    "    for (checkpoint, model_name) in zip(checkpoints, model_names)\n",
    "    if checkpoint[\"configuration\"][\"method\"] == \"reduction\"\n",
    "]\n",
    "unconstrained_checkpoints = [\n",
    "    (checkpoint, model_name)\n",
    "    for (checkpoint, model_name) in zip(checkpoints, model_names)\n",
    "    if checkpoint[\"configuration\"][\"method\"] == \"unconstrained\"\n",
    "]\n",
    "constrained_checkpoints = [\n",
    "    (checkpoint, model_name)\n",
    "    for (checkpoint, model_name) in zip(checkpoints, model_names)\n",
    "    if checkpoint[\"configuration\"][\"method\"] == \"constrained\"\n",
    "]\n",
    "soft_constrained_checkpoints = [\n",
    "    (checkpoint, model_name)\n",
    "    for (checkpoint, model_name) in zip(checkpoints, model_names)\n",
    "    if checkpoint[\"configuration\"][\"method\"] == \"soft-constrained\"\n",
    "]\n",
    "\n",
    "\n",
    "tasks = [\n",
    "    (\"Final Models\", final_checkpoints),\n",
    "    #     (\"Approximation Interpolation\", interpolation_task(approximation_checkpoints)),\n",
    "    #     (\"Approximation Extrapolation\", extrapolation_task(approximation_checkpoints)),\n",
    "    #     (\"Reduction Interpolation\", interpolation_task(reduction_checkpoints)),\n",
    "    #     (\"Reduction Extrapolation\", extrapolation_task(reduction_checkpoints)),\n",
    "    #     (\"Unconstrained Interpolation\", interpolation_task(unconstrained_checkpoints)),\n",
    "    #     (\"Unconstrained Extrapolation\", extrapolation_task(unconstrained_checkpoints)),\n",
    "    #     (\"Soft-Constrained Interpolation\", interpolation_task(soft_constrained_checkpoints)),\n",
    "    #     (\"Soft-Constrained Extrapolation\", extrapolation_task(soft_constrained_checkpoints)),\n",
    "    #     (\"Constrained Interpolation\", interpolation_task(constrained_checkpoints)),\n",
    "    #     (\"Constrained Extrapolation\", extrapolation_task(constrained_checkpoints)),\n",
    "]\n",
    "\n",
    "for task_name, task in tasks:\n",
    "    print(task_name)\n",
    "    if len(task) == 0:\n",
    "        print(f\"Nothing for task {task_name}\")\n",
    "        continue\n",
    "    task_checkpoints = [x[0] for x in task]\n",
    "    task_model_names = [x[1] for x in task]\n",
    "    task_monitors = [checkpoint[\"monitors\"] for checkpoint in task_checkpoints]\n",
    "    time_keys = set()\n",
    "    for monitors in task_monitors:\n",
    "        time_keys.update(\n",
    "            [\n",
    "                key\n",
    "                for key in monitors[0].timing[0][0].keys()\n",
    "                if (\"multipliers\" in key or \"total\" in key or \"compute\" in key)\n",
    "                and \"error\" not in key\n",
    "                and \"recomputed\" not in key\n",
    "            ]\n",
    "        )\n",
    "    time_keys = list(time_keys)\n",
    "    task_filename = convert_name_to_filename(task_name)\n",
    "\n",
    "    if \"final\" in task_filename:\n",
    "        fig = plot_time(\n",
    "            [monitors[0] for monitors in task_monitors],\n",
    "            task_model_names,\n",
    "            f\"{task_filename}_compute-time\",\n",
    "            time_keys=time_keys,\n",
    "            log=True,\n",
    "            directory=save_directory,\n",
    "        )\n",
    "\n",
    "        fig = plot_loss(\n",
    "            [(monitors[1], None) for monitors in task_monitors],\n",
    "            task_model_names,\n",
    "            f\"{task_filename}_constrained-loss\",\n",
    "            title=\"Constrained Loss\",\n",
    "            log=True,\n",
    "            directory=save_directory,\n",
    "            constrained=True,\n",
    "        )\n",
    "        fig = plot_constraints(\n",
    "            [(monitors[1], None) for monitors in task_monitors],\n",
    "            task_model_names,\n",
    "            f\"{task_filename}_constraint\",\n",
    "            title=\"Constraint Magnitude\",\n",
    "            log=True,\n",
    "            directory=save_directory,\n",
    "        )\n",
    "        fig = plot_loss(\n",
    "            [(monitors[1], None) for monitors in task_monitors],\n",
    "            task_model_names,\n",
    "            f\"{task_filename}_loss\",\n",
    "            title=\"Loss\",\n",
    "            log=True,\n",
    "            directory=save_directory,\n",
    "        )\n",
    "\n",
    "        data, predictions = get_model_predictions(task_checkpoints)\n",
    "        fig = plot_model_predictions(\n",
    "            data,\n",
    "            predictions,\n",
    "            task_model_names,\n",
    "            f\"{task_filename}_predictions\",\n",
    "            title=\"Model Predictions\",\n",
    "            directory=save_directory,\n",
    "        )\n",
    "\n",
    "    if (\n",
    "        \"approximation\" in task_filename\n",
    "        or \"reduction\" in task_filename\n",
    "        or \"constrained\" in task_filename\n",
    "    ):\n",
    "\n",
    "        idxs = np.argsort([checkpoint[\"epoch\"] for checkpoint in task_checkpoints])\n",
    "        task_checkpoints = np.array(task_checkpoints)[idxs]\n",
    "        task_model_names = np.array(task_model_names)[idxs]\n",
    "        task_monitors = np.array(task_monitors)[idxs]\n",
    "\n",
    "        fig = plot_loss(\n",
    "            [monitors[1:] for monitors in task_monitors[-1:]],\n",
    "            [task_name],\n",
    "            f\"{task_filename}_constrained-loss\",\n",
    "            title=f\"{task_name} Constrained Loss\",\n",
    "            directory=save_directory,\n",
    "            constrained=True,\n",
    "        )\n",
    "        fig = plot_constraints(\n",
    "            [monitors[1:] for monitors in task_monitors[-1:]],\n",
    "            [task_name],\n",
    "            f\"{task_filename}_constraint\",\n",
    "            title=f\"{task_name} Constraint Magnitude\",\n",
    "            directory=save_directory,\n",
    "        )\n",
    "        fig = plot_loss(\n",
    "            [monitors[1:] for monitors in task_monitors[-1:]],\n",
    "            [task_name],\n",
    "            f\"{task_filename}_loss\",\n",
    "            title=f\"{task_name} Loss\",\n",
    "            directory=save_directory,\n",
    "        )\n",
    "\n",
    "        extra_idxs = np.power(1.5, np.arange(len(task_checkpoints))).astype(int)\n",
    "        extra_idxs = extra_idxs[extra_idxs < len(task_checkpoints)]\n",
    "        limited_idxs = np.unique(np.r_[0, extra_idxs, len(task_checkpoints) - 1])\n",
    "        limited_checkpoints = task_checkpoints[limited_idxs]\n",
    "        data, predictions = get_model_predictions(limited_checkpoints)\n",
    "        fig = plot_model_predictions(\n",
    "            data,\n",
    "            predictions,\n",
    "            [f'Epoch {checkpoint[\"epoch\"]}' for checkpoint in limited_checkpoints],\n",
    "            f\"{task_filename}_predictions\",\n",
    "            title=task_name,\n",
    "            directory=save_directory,\n",
    "        )\n",
    "\n",
    "    # # close all those figures\n",
    "    # plt.close(\"all\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:.conda-torch-gpu]",
   "language": "python",
   "name": "conda-env-.conda-torch-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
