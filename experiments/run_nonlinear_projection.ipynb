{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to the root so that relative path loads work correctly\n",
    "import os\n",
    "\n",
    "try:\n",
    "    os.chdir(os.path.join(os.getcwd(), \"..\"))\n",
    "    print(os.getcwd())\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from experiments.C_nonlinear_projection.constraints import (\n",
    "    helmholtz_equation,\n",
    "    pythagorean_equation,\n",
    "    truth_residual,\n",
    ")\n",
    "from experiments.C_nonlinear_projection.error_functions import Huber_Error, Lp_Error\n",
    "from experiments.C_nonlinear_projection.experiment_definition import dictionary_product\n",
    "from experiments.C_nonlinear_projection.main import run_experiment\n",
    "from experiments.C_nonlinear_projection.model import Dense, ParameterizedDense, Swish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving utilities\n",
    "def get_savefile():\n",
    "    base_name = \"nonlinear-projection\"\n",
    "    time_string = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    savefile = f\"{base_name}_{time_string}.pth\"\n",
    "    return savefile\n",
    "\n",
    "\n",
    "save_directory = os.path.expandvars(\n",
    "    \"$SCRATCH/results/checkpoints/C_nonlinear_projection\"\n",
    ")\n",
    "\n",
    "\n",
    "def save_out(summary, savefile, directory=save_directory):\n",
    "    full_file = f\"{directory}/{savefile}\"\n",
    "    print(f\"Saving to file {full_file}\")\n",
    "    torch.save(summary, full_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_configuration = {\n",
    "    \"training_parameterizations\": {\n",
    "        \"amplitudes\": np.linspace(0.2, 5.0, num=10),\n",
    "        \"frequencies\": np.linspace(0.2, 5.0, num=10),\n",
    "        \"phases\": np.linspace(-0.5, 0.5, num=10),\n",
    "        \"num_points\": 500,\n",
    "        \"sampling\": \"random\",\n",
    "    },\n",
    "    \"testing_parameterizations\": {\n",
    "        \"amplitudes\": [1.0],\n",
    "        \"frequencies\": [1.0],\n",
    "        \"phases\": [0.0],\n",
    "        \"num_points\": 100,\n",
    "        \"sampling\": \"uniform\",\n",
    "    },\n",
    "    \"batch_size\": 1000,\n",
    "    \"projection_batch_size\": 50,\n",
    "    \"architecture\": ParameterizedDense,\n",
    "    \"model_size\": [50, 50, 50, 50, 50],\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"model_act\": Swish(),\n",
    "    \"device\": \"cpu\",\n",
    "    \"constraint\": helmholtz_equation,\n",
    "    \"error_fn\": None,\n",
    "    \"regularization_weight\": 0,\n",
    "    \"tolerance\": 0,  # Force that we always use max iterations projection steps\n",
    "    \"max_iterations\": 1e4,\n",
    "}\n",
    "\n",
    "configuration_revisions = list(\n",
    "    dictionary_product(\n",
    "        **{\n",
    "            \"max_iterations\": [1e5],\n",
    "            \"tolerance\": [1e-3],\n",
    "            \"regularization_weight\": [0.0],\n",
    "            \"learning_rate\": [1e-3],\n",
    "            \"projection_learning_rate\": [1e-4],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "num_epochs = 100\n",
    "save_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment\n",
    "all_savefiles = list()\n",
    "final_checkpoints = list()\n",
    "for revision in configuration_revisions:\n",
    "    configuration = base_configuration.copy()\n",
    "    configuration.update(revision)\n",
    "\n",
    "    savefile = get_savefile()\n",
    "    all_savefiles.append(savefile)\n",
    "    print(f\"Running proof of constraint with savefile {savefile}\")\n",
    "    checkpoint_save_file_base = os.path.splitext(savefile)[0]\n",
    "    final_checkpoints.append(f\"{checkpoint_save_file_base}_{num_epochs:05d}.pth\")\n",
    "    final_result = run_experiment(\n",
    "        num_epochs,\n",
    "        log=print,\n",
    "        save_directory=save_directory,\n",
    "        save_file=checkpoint_save_file_base,\n",
    "        save_interval=save_interval,\n",
    "        evaluate=True,\n",
    "        inference=True,\n",
    "        **configuration,\n",
    "    )\n",
    "    print(f\"Completed run with savefile {savefile}\")\n",
    "print(\"\")\n",
    "print(f\"Files were saved to {all_savefiles}\")\n",
    "print(f\"Checkpoints were saved to {final_checkpoints}\")\n",
    "print(f\"Corresponding revisions {configuration_revisions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done!\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:.conda-torch3]",
   "language": "python",
   "name": "conda-env-.conda-torch3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
