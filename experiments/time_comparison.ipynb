{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to the root so that relative path loads work correctly\n",
    "import os\n",
    "\n",
    "try:\n",
    "    os.chdir(os.path.join(os.getcwd(), \"..\"))\n",
    "    print(os.getcwd())\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from experiments.A_proof_of_constraint.experiment_definition import dictionary_product\n",
    "from experiments.A_proof_of_constraint.main import (\n",
    "    build_model_and_optimizer,\n",
    "    run_experiment,\n",
    ")\n",
    "from experiments.A_proof_of_constraint.model import Dense, ParameterizedDense\n",
    "from experiments.A_proof_of_constraint.reductions import Huber_Reduction, Lp_Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving utilities\n",
    "def get_savefile(configuration):\n",
    "    base_name = experiment_name\n",
    "    method = configuration[\"method\"]\n",
    "    model_size = configuration[\"model_size\"][0]\n",
    "    batch_size = configuration[\"batch_size\"]\n",
    "    savefile = f\"{base_name}_{method}_batchsize{batch_size}_modelsize{model_size}.pth\"\n",
    "    return savefile\n",
    "\n",
    "\n",
    "experiment_name = \"timing\"\n",
    "save_directory = f\"results/checkpoints/{experiment_name}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_configuration = {\n",
    "    \"training_parameterizations\": {\n",
    "        \"amplitudes\": [1.0],\n",
    "        \"frequencies\": [1.0],\n",
    "        \"phases\": [0.0],\n",
    "        \"num_points\": None,\n",
    "        \"sampling\": \"uniform\",\n",
    "    },\n",
    "    \"testing_parameterizations\": {\n",
    "        \"amplitudes\": [1.0],\n",
    "        \"frequencies\": [1.0],\n",
    "        \"phases\": [0.0],\n",
    "        \"num_points\": 1,\n",
    "        \"sampling\": \"uniform\",\n",
    "    },\n",
    "    \"batch_size\": 1000,\n",
    "    \"architecture\": Dense,\n",
    "    \"model_size\": [50, 50, 50, 50, 50],\n",
    "    \"method\": \"constrained\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"ground_approximation\": None,\n",
    "    \"reduction\": Huber_Reduction(6),\n",
    "    \"model_act\": nn.Tanh(),\n",
    "}\n",
    "\n",
    "\n",
    "def fix_configuration(configuration):\n",
    "    # Sets things so we have exactly 100 iterations in the single epoch\n",
    "    configuration[\"training_parameterizations\"][\"num_points\"] = (\n",
    "        configuration[\"batch_size\"] * 100\n",
    "    )\n",
    "    return configuration\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "save_interval = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of experiment\n",
    "configuration_revisions = list()\n",
    "\n",
    "# sub_experiment: batch_size\n",
    "configuration_revisions.extend(\n",
    "    dictionary_product(\n",
    "        **{\n",
    "            \"method\": [\"unconstrained\", \"soft-constrained\", \"reduction\", \"constrained\"],\n",
    "            #             \"method\": [\"unconstrained\", \"soft-constrained\", \"reduction\"],\n",
    "            \"batch_size\": [10, 50, 100, 500, 1000],\n",
    "            \"model_size\": [[100]],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "# sub_experiment: model_size (without duplication of the batch_size=100, model_size=100 case)\n",
    "configuration_revisions.extend(\n",
    "    dictionary_product(\n",
    "        **{\n",
    "            \"method\": [\"unconstrained\", \"soft-constrained\", \"reduction\", \"constrained\"],\n",
    "            #             \"method\": [\"unconstrained\", \"soft-constrained\", \"reduction\"],\n",
    "            \"batch_size\": [100],\n",
    "            \"model_size\": [[10], [50], [500], [1000]],\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: the experiment will take a few hours to run on a standard processor\n",
    "rerun_experiment = False\n",
    "\n",
    "# Delete old saves\n",
    "if rerun_experiment:\n",
    "    files = glob.glob(f\"{save_directory}/*.pth\")\n",
    "    for f in files:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment\n",
    "\n",
    "\n",
    "def dont_print(*args):\n",
    "    # Literally do nothing and let the print statement die\n",
    "    pass\n",
    "\n",
    "\n",
    "if rerun_experiment:\n",
    "    final_checkpoints = list()\n",
    "    for revision in configuration_revisions:\n",
    "        configuration = base_configuration.copy()\n",
    "        configuration.update(revision)\n",
    "        configuration = fix_configuration(configuration)\n",
    "\n",
    "        savefile = get_savefile(configuration)\n",
    "        print(f\"Running proof of constraint with savefile {savefile}\")\n",
    "        checkpoint_save_file_base = os.path.splitext(savefile)[0]\n",
    "        final_checkpoints.append(f\"{checkpoint_save_file_base}_{num_epochs:05d}.pth\")\n",
    "        final_result = run_experiment(\n",
    "            num_epochs,\n",
    "            evaluate_training=False,  # we only care about training time\n",
    "            evaluate_testing=False,\n",
    "            log=dont_print,\n",
    "            save_directory=save_directory,\n",
    "            save_file=checkpoint_save_file_base,\n",
    "            save_interval=save_interval,\n",
    "            **configuration,\n",
    "        )\n",
    "        print(f\"Completed run with savefile {savefile}\")\n",
    "    print(\"\")\n",
    "    print(f\"Checkpoints were saved to {final_checkpoints}\")\n",
    "    print(f\"Corresponding revisions {configuration_revisions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize experiment\n",
    "from experiments.A_proof_of_constraint.visualize import plot_time_experiment\n",
    "\n",
    "\n",
    "def get_model_name(checkpoint):\n",
    "    method = checkpoint[\"configuration\"][\"method\"]\n",
    "    return f\"{method}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "files = glob.glob(f\"{save_directory}/*.pth\")\n",
    "files.sort()\n",
    "checkpoints = [torch.load(f) for f in files]\n",
    "model_names = [get_model_name(checkpoint) for checkpoint in checkpoints]\n",
    "print(model_names)\n",
    "# Make sure directory to save exists\n",
    "plot_directory = f\"/global/u1/g/gelijerg/Projects/pyinsulate/results/{experiment_name}/\"\n",
    "os.makedirs(plot_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the checkpoints for each sub-experiment\n",
    "def get_batch_size_experiment(checkpoints, model_names, mask_out=[]):\n",
    "    configurations = np.array(\n",
    "        [checkpoint[\"configuration\"] for checkpoint in checkpoints]\n",
    "    )\n",
    "    mask = np.array(\n",
    "        [configuration[\"model_size\"][0] == 100 for configuration in configurations]\n",
    "    )\n",
    "    idxs = np.argsort(\n",
    "        [configuration[\"batch_size\"] for configuration in configurations[mask]]\n",
    "    )\n",
    "    exp_checkpoints = np.array(checkpoints)[mask][idxs]\n",
    "    exp_model_names = np.array(model_names)[mask][idxs]\n",
    "    grouped_checkpoints = list()\n",
    "    # reverse the sorting so that unconstrained comes first\n",
    "    grouped_labels = np.sort(np.unique(exp_model_names))[::-1]\n",
    "    grouped_labels = [label for label in grouped_labels if label not in mask_out]\n",
    "    for unique_name in grouped_labels:\n",
    "        grouped_checkpoints.append(exp_checkpoints[exp_model_names == unique_name])\n",
    "    return grouped_checkpoints, grouped_labels\n",
    "\n",
    "\n",
    "def get_model_size_experiment(checkpoints, model_names, mask_out=[]):\n",
    "    configurations = np.array(\n",
    "        [checkpoint[\"configuration\"] for checkpoint in checkpoints]\n",
    "    )\n",
    "    mask = np.array(\n",
    "        [configuration[\"batch_size\"] == 100 for configuration in configurations]\n",
    "    )\n",
    "    idxs = np.argsort(\n",
    "        [configuration[\"model_size\"][0] for configuration in configurations[mask]]\n",
    "    )\n",
    "    exp_checkpoints = np.array(checkpoints)[mask][idxs]\n",
    "    exp_model_names = np.array(model_names)[mask][idxs]\n",
    "    grouped_checkpoints = list()\n",
    "    # reverse the sorting so that unconstrained comes first\n",
    "    grouped_labels = np.sort(np.unique(exp_model_names))[::-1]\n",
    "    grouped_labels = [label for label in grouped_labels if label not in mask_out]\n",
    "    for unique_name in grouped_labels:\n",
    "        grouped_checkpoints.append(exp_checkpoints[exp_model_names == unique_name])\n",
    "    return grouped_checkpoints, grouped_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(checkpoint):\n",
    "    # gets how many parameters the model actually has\n",
    "    model, opt = build_model_and_optimizer(checkpoint[\"configuration\"])\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually plot\n",
    "batch_size_experiment_checkpoints, batch_size_experiment_model_names = get_batch_size_experiment(\n",
    "    checkpoints, model_names\n",
    ")\n",
    "fig = plot_time_experiment(\n",
    "    [\n",
    "        [checkpoint[\"monitors\"][0] for checkpoint in group]\n",
    "        for group in batch_size_experiment_checkpoints\n",
    "    ],\n",
    "    batch_size_experiment_model_names,\n",
    "    xvalues=[\n",
    "        checkpoint[\"configuration\"][\"batch_size\"]\n",
    "        for checkpoint in batch_size_experiment_checkpoints[0]\n",
    "    ],\n",
    "    savefile=f\"batch_size_dependence_all\",\n",
    "    title=\"Training time scaling with batch size\",\n",
    "    ylabel=\"Seconds per iteration\",\n",
    "    xlabel=\"Batch size\",\n",
    "    confidence_interval=95.0,\n",
    ")\n",
    "batch_size_experiment_checkpoints, batch_size_experiment_model_names = get_batch_size_experiment(\n",
    "    checkpoints, model_names, mask_out=[\"constrained\"]\n",
    ")\n",
    "fig = plot_time_experiment(\n",
    "    [\n",
    "        [checkpoint[\"monitors\"][0] for checkpoint in group]\n",
    "        for group in batch_size_experiment_checkpoints\n",
    "    ],\n",
    "    batch_size_experiment_model_names,\n",
    "    xvalues=[\n",
    "        checkpoint[\"configuration\"][\"batch_size\"]\n",
    "        for checkpoint in batch_size_experiment_checkpoints[0]\n",
    "    ],\n",
    "    savefile=f\"batch_size_dependence_small\",\n",
    "    title=\"Training time scaling with batch size\",\n",
    "    ylabel=\"Seconds per iteration\",\n",
    "    xlabel=\"Batch size\",\n",
    "    confidence_interval=95.0,\n",
    ")\n",
    "\n",
    "model_size_experiment_checkpoints, model_size_experiment_model_names = get_model_size_experiment(\n",
    "    checkpoints, model_names\n",
    ")\n",
    "fig = plot_time_experiment(\n",
    "    [\n",
    "        [checkpoint[\"monitors\"][0] for checkpoint in group]\n",
    "        for group in model_size_experiment_checkpoints\n",
    "    ],\n",
    "    model_size_experiment_model_names,\n",
    "    xvalues=[\n",
    "        get_model_size(checkpoint)\n",
    "        for checkpoint in model_size_experiment_checkpoints[0]\n",
    "    ],\n",
    "    savefile=f\"model_size_dependence_all\",\n",
    "    title=\"Training time scaling with model size\",\n",
    "    ylabel=\"Seconds per iteration\",\n",
    "    xlabel=\"Number of trainable paramters\",\n",
    "    confidence_interval=95.0,\n",
    ")\n",
    "model_size_experiment_checkpoints, model_size_experiment_model_names = get_model_size_experiment(\n",
    "    checkpoints, model_names, mask_out=[\"constrained\"]\n",
    ")\n",
    "fig = plot_time_experiment(\n",
    "    [\n",
    "        [checkpoint[\"monitors\"][0] for checkpoint in group]\n",
    "        for group in model_size_experiment_checkpoints\n",
    "    ],\n",
    "    model_size_experiment_model_names,\n",
    "    xvalues=[\n",
    "        get_model_size(checkpoint)\n",
    "        for checkpoint in model_size_experiment_checkpoints[0]\n",
    "    ],\n",
    "    savefile=f\"model_size_dependence_small\",\n",
    "    title=\"Training time scaling with model size\",\n",
    "    ylabel=\"Seconds per iteration\",\n",
    "    xlabel=\"Number of trainable paramters\",\n",
    "    confidence_interval=95.0,\n",
    ")\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:.conda-torch-gpu]",
   "language": "python",
   "name": "conda-env-.conda-torch-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
